

<!doctype html>



<html>

<head>

  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">

  <meta name="theme-color" content="#4F7DC9">

  <meta charset="UTF-8">

  <title>Building a basic GPU accelerated application with OpenMP in C/C&#43;&#43;</title>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">

  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">

  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">

  <style>

    .success {

      color: #1e8e3e;

    }

    .error {

      color: red;

    }

  </style>


<!-- update the version number as needed -->
<script defer src="/__/firebase/7.9.3/firebase-app.js"></script>
<!-- include only the Firebase features as you need -->
<script defer src="/__/firebase/7.9.3/firebase-auth.js"></script>
<script defer src="/__/firebase/7.9.3/firebase-database.js"></script>
<script defer src="/__/firebase/7.9.3/firebase-messaging.js"></script>
<script defer src="/__/firebase/7.9.3/firebase-storage.js"></script>
<!-- initialize the SDK after all desired features are loaded -->
<script defer src="/__/firebase/init.js"></script>

</head>

<body>

  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>

  <google-codelab codelab-gaid="TO DO"

                  id="URL"

                  title="Building a basic GPU accelerated application with OpenMP in C/C&#43;&#43;"

                  environment="web"

                  feedback-link="TO DO">

    

      <google-codelab-step label="Introduction" duration="0">

        <p><strong>Last Updated:</strong> 2020-18-09</p>

<p>Over the last few decades, there has been increased interest in using Graphics Processing Units (GPUs) to perform general purpose computing tasks. This practice is often referred to as General Purpose GPU (GPGPU) programming. Using GPUs for general purpose computing tasks gained attention primarily due to the inherent scale of parallelism within GPU hardware that enables faster computation and a reduction in the time-to-solution.</p>

<p>Since GPUs were designed for handling graphics rendering tasks, implementing general purpose routines, like those for numerically solving partial differential equation or optimizing the weights in a neural network, was time-consuming and often error-prone. The continued interest and success of GPGPU computing led to the development of more user-friendly application programming interfaces (APIs) that allow developers to focus more attention on implementing algorithms using the syntax of compiled languages they are more familiar with, rather than thinking about how to express their algorithm in terms of graphics operations.</p>

<p>Before diving into GPU programming APIs and how they can help you accelerate scientific applications, let&#39;s first discuss the basics of modern GPU accelerated compute platforms to help you better understand the software development problems they help solve.</p>

<h2 is-upgraded><strong>Basics of GPU Accelerated Platforms</strong></h2>

<p>A GPU is an additional hardware component that can perform operations alongside a CPU. GPUs are either integrated into the motherboard or silicon dye alongside a CPU, or are made available through a dedicated interconnect, called the Peripheral Component Interconnect (PCI). The PCI is a physical hardware component that allows data to be transmitted between the CPU and GPU.</p>

<p class="image-container"><img style="width: 348.50px" src="img/eb4f4a95fc0a22c0.png"></p>

<p>On GPU-Accelerated High Performance Computing platforms, you will primarily encounter servers with one or more dedicated GPUs. Dedicated GPUs have an isolated set of compute cores and their own memory space, distinct from the CPU and the CPU&#39;s memory space. The figure above illustrates simple conceptual model of a server with a CPU connected to a single GPU. This conceptual model is purposefully simplified to highlight the first hurdle that all new GPU developers must overcome : managing CPU and GPU memory spaces.</p>

<p>On most modern GPU accelerated platforms, migrating data between the CPU and GPU can be a bottleneck. This is caused by limits in the PCI Bus peak bandwidth. Because of this, developers must be mindful to minimize the amount of data transfer between CPU and GPU for optimal performance.</p>

<p>A more subtle aspect of GPU programming, driven by the fact that a GPU is a separate hardware component from the CPU, is the potential for asynchronous activities between the CPU and GPU. When developing a GPU accelerated application, kernels that can execute on the GPU are scheduled for execution by the CPU. Most modern GPU programming APIs provide calls that can force the issuing CPU process to stop and wait for the GPU kernel execution. Further, when a CPU issues multiple kernel execution instructions, these APIs can allow for serialized or asynchronous executions.</p>

<h3 is-upgraded><strong>GPU Hardware</strong></h3>

<p>GPUs for high performance computing are available from three different vendors : AMD, Nvidia, and Intel. Currently, each has their own terminology for describing the architecture and microarchitecture. We&#39;ll briefly describe a conceptual model of a GPU and relate terminology between vendors.<img style="width: 600.00px" src="img/c4854d064849fbc1.png"></p>

<p>In general, GPUs are comprised of a number of compute units (AMD) or streaming multiprocessors (Nvidia), &#34;Global&#34; GPU RAM, and a Work Group Distributor (AMD) or Workload Manager (Nvidia). On Nvidia hardware, the Workload Manager schedules work to the streaming multiprocessors, which have a Same-Instruction-Multiple-Thread (SIMT) scheduler, Cache memory, registers, and a set of CUDA Cores. On AMD Hardware, the Work Group Distributor schedules work across the compute units, which each have a scheduler, local data share, L1 Cache, a mix of scalar and vector registers, and a Vector Arithmetic Logic Unit (ALU). </p>

<p>GPU models are distinguished based on their microarchitecture and other characteristics, such as the number of compute units, PCI compatibility, memory and compute clock frequencies, and global memory size. For AMD GPUs, the microarchitecture refers to the architecture of the compute units.</p>

<p>Below is a conceptual diagram of a single compute unit in AMD&#39;s Vega 20 micro-architecture. This micro-architecture is at the core of the Radeon Instinct MI50 &amp; MI60 GPUs and the <a href="https://www.amd.com/en/products/exascale-era%5C" target="_blank">Department of Energy&#39;s newest exascale systems</a>, <a href="https://www.llnl.gov/news/llnl-and-hpe-partner-amd-el-capitan-projected-worlds-fastest-supercomputer" target="_blank">El Capitan</a> and <a href="https://www.olcf.ornl.gov/frontier/" target="_blank">Frontier</a>. On AMD GPUs, each compute unit has 64 Vector ALU&#39;s.<img style="width: 624.00px" src="img/605439049bb81405.jpeg"></p>

<p>The table below summarizes some of the characteristics of a few of the <a href="https://www.amd.com/en/graphics/servers-radeon-instinct-mi" target="_blank">latest lineup of AMD Radeon Instinct GPUs</a>. In this table, we are showing GPUs with varying microarchitecture, number of compute units, and global GPU memory size. The GPU&#39;s memory clock rate and compute clock rate, together with the type of memory and the number of Vector ALU&#39;s dictate the peak performance and memory bandwidth.</p>

<table>

<tr><td colspan="1" rowspan="1"><p><a href="https://www.amd.com/en/products/professional-graphics/instinct-mi50-32gb" target="_blank"><strong>MI50</strong></a></p>

</td><td colspan="1" rowspan="1"><p><a href="https://www.amd.com/en/products/professional-graphics/instinct-mi25" target="_blank"><strong>MI25</strong></a></p>

</td><td colspan="1" rowspan="1"><p><a href="https://www.amd.com/en/products/professional-graphics/instinct-mi8" target="_blank"><strong>MI8</strong></a></p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Microarchitecture</strong></p>

</td><td colspan="1" rowspan="1"><p>Vega20</p>

</td><td colspan="1" rowspan="1"><p>Vega10</p>

</td><td colspan="1" rowspan="1"><p>Fiji</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Compute Units</strong></p>

</td><td colspan="1" rowspan="1"><p>60</p>

</td><td colspan="1" rowspan="1"><p>64</p>

</td><td colspan="1" rowspan="1"><p>64</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Peak FP16 </strong></p>

</td><td colspan="1" rowspan="1"><p>26.5 TFLOPS</p>

</td><td colspan="1" rowspan="1"><p>24.6 TFLOPS</p>

</td><td colspan="1" rowspan="1"><p>8.19 TFLOPS</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Peak FP32 </strong></p>

</td><td colspan="1" rowspan="1"><p>13.3 TFLOPS</p>

</td><td colspan="1" rowspan="1"><p>12.29 TFLOPS</p>

</td><td colspan="1" rowspan="1"><p>8.19 TFLOPS</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Peak FP64 </strong></p>

</td><td colspan="1" rowspan="1"><p>6.6 TFLOPS</p>

</td><td colspan="1" rowspan="1"><p>768 GFLOPS</p>

</td><td colspan="1" rowspan="1"><p>512 GFLOPS</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Memory Size</strong></p>

</td><td colspan="1" rowspan="1"><p>16-32 GB (HBM2)</p>

</td><td colspan="1" rowspan="1"><p>16 GB (HBM2)</p>

</td><td colspan="1" rowspan="1"><p>4 GB (HBM)</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><strong>Memory Bandwidth</strong></p>

</td><td colspan="1" rowspan="1"><p>1 TB/s</p>

</td><td colspan="1" rowspan="1"><p>484 GB/s</p>

</td><td colspan="1" rowspan="1"><p>512 GB/s</p>

</td></tr>

</table>

<p>Now that you have some awareness of GPU hardware, let&#39;s talk about how we program GPUs to accelerate scientific applications.</p>

<h2 is-upgraded><strong>GPU Programming APIs</strong></h2>

<p>In general, a GPU programming API must provide routines that developers can leverage to allocate and deallocate memory on the GPU, copy memory between the CPU and GPU, and control kernel execution. GPU programming APIs can be classified into two categories</p>

<ol type="1" start="1">

<li>Directive-Based</li>

<li>Kernel-Based</li>

</ol>

<p>When programming with Directive-Based APIs, developers will provide &#34;hints&#34; to the compiler about how to offload sections of code to the GPU. In this approach, the compiler will then generate code for allocating/deallocating memory, copying memory between host and device, and how to parallelize sections of code. This method of GPU programming has the benefit of being able to start running on GPUs quickly with little effort. Additionally, management of CPU and GPU memory is handled &#34;behind-the-scenes&#34; by the compiler and can help limit code complexity. In this case, compilers will more often make decisions that ensure correctness, rather than optimize performance. Because of this, performance tuning often requires verbose compiler hints to limit superfluous data transfer between CPU and GPU and sometimes require alteration of the CPU code.</p>

<p>When programming with Kernel-Based APIs, developers are solely responsible for creating and managing both CPU and GPU memory spaces. Additionally, developers must write compute kernels that are consistent with their CPU counterparts and issue explicit calls to launch routines when needed. While this approach increases code complexity and has a higher barrier to entry than Directive-Based approaches, the developer has precise control over the performance of GPU kernels. Additionally, developers can control when data transfers between CPU and GPU occur, allowing for a clear path to minimize time spent crossing the PCI Bus.</p>

<p>The table below provides a breakdown of popular GPU programming APIs, their type, which compilers expose the API, and which GPU platforms the API allows you to program for.  It&#39;s important to keep in mind that directive-based APIs yield varied performance across compilers. Further, Fortran compilers that are 2003 compliant and above are able to leverage <a href="http://fortranwiki.org/fortran/show/iso_c_binding" target="_blank">ISO_C_BINDING</a> to expose C/C++ routines that can be called from Fortran source code, allowing C/C++ APIs to be made available in Fortran through C-interoperability.</p>

<table>

<tr><td colspan="1" rowspan="1"><p><strong>API </strong></p>

</td><td colspan="1" rowspan="1"><p><strong>Type</strong></p>

</td><td colspan="1" rowspan="1"><p><strong>Compiler Support</strong></p>

</td><td colspan="1" rowspan="1"><p><strong>Platforms</strong></p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><a href="https://github.com/ROCm-Developer-Tools/HIP" target="_blank">HIP</a> &amp; <a href="https://github.com/ROCmSoftwarePlatform/hipfort" target="_blank">hipfort</a></p>

</td><td colspan="1" rowspan="1"><p>Kernel</p>

</td><td colspan="1" rowspan="1"><p>Hipcc (hcc/nvcc)</p>

</td><td colspan="1" rowspan="1"><p>AMD, Nvidia</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><a href="https://www.openmp.org/specifications/" target="_blank">OpenMP (v5.0)</a></p>

</td><td colspan="1" rowspan="1"><p>Directive</p>

</td><td colspan="1" rowspan="1"><p>AOMP (Clang/Flang), GCC 10, XL</p>

</td><td colspan="1" rowspan="1"><p>AMD, Nvidia</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p>CUDA</p>

</td><td colspan="1" rowspan="1"><p>Kernel</p>

</td><td colspan="1" rowspan="1"><p>nvcc</p>

</td><td colspan="1" rowspan="1"><p>Nvidia</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p>CUDA-Fortran</p>

</td><td colspan="1" rowspan="1"><p>Kernel</p>

</td><td colspan="1" rowspan="1"><p>PGI</p>

</td><td colspan="1" rowspan="1"><p>Nvidia</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p>OpenACC</p>

</td><td colspan="1" rowspan="1"><p>Directive</p>

</td><td colspan="1" rowspan="1"><p>GCC 9, PGI, XL</p>

</td><td colspan="1" rowspan="1"><p>Nvidia</p>

</td></tr>

<tr><td colspan="1" rowspan="1"><p><a href="https://www.khronos.org/opencl/" target="_blank">OpenCL</a></p>

</td><td colspan="1" rowspan="1"><p>Kernel</p>

</td><td colspan="1" rowspan="1"><p>All</p>

</td><td colspan="1" rowspan="1"><p>All</p>

</td></tr>

</table>

<h3 is-upgraded><strong>ROCm, HIP, and OpenMP : Portable, Open-Source Platforms for GPU Acceleration</strong></h3>

<p>AMD, Nvidia, and Intel all design and manufacture GPUs for High Performance Computing. Currently, there is no unified machine code for GPUs that all vendors currently support on the hardware they produce. This has resulted in portability issues and the common &#34;vendor-lock&#34; problem, where HPC developers spend a significant amount of effort to port their application to a specific GPU and then lose the ability to easily transition to other hardware.</p>

<p>As we have just shown, there are a number of APIs available that support GPGPU programming. Currently, this ecosystem is at a turning point where APIs are shifting towards meeting open-source and portability standards that enable developers to leverage GPU hardware from multiple vendors and even multi-core CPU platforms all with the same code. </p>

<h4 is-upgraded>ROCm</h4>

<p>AMD is currently leading this effort through its <a href="https://www.amd.com/en/graphics/servers-solutions-rocm" target="_blank">ROCm platform</a>. ROCm is AMD&#39;s open source platform for GPU accelerated computing that covers everything from the device driver and runtimes, to compilers, programming models and libraries. It also supports different frameworks and applications and comes with a complete set of developer tools for debugging and profiling your application to help you get the best possible performance.</p>

<p>The diagram below summarizes the ROCm ecosystem that helps bridge the gap between HPC and Machine Learning applications and the variety of compute hardware targets, including GPUs.<img style="width: 624.00px" src="img/1a114e258c2c2c69.png"></p>

<h4 is-upgraded>HIP</h4>

<p>For Kernel-based GPU programming, ROCm includes the <a href="https://rocmdocs.amd.com/en/latest/Installation_Guide/HIP.html" target="_blank">Heterogeneous-Compute Interface for Portability (HIP)</a> and OpenCL. The Heterogeneous Interface for Portability (HIP) is AMD&#39;s dedicated GPU programming environment for designing high performance kernels on GPU hardware. AMD provides hipify tools that will convert CUDA to HIP, enhancing the performance portability of your GPU accelerated applications. The interface design of the API allows your new single source application to be compiled to target either AMD or NV hardware.</p>

<p>HIP is a C++ dialect, similar to CUDA, that allows for programming and AMD and Nvidia GPUs. HIP maintainers have plans to support Intel (XE) GPUs in future releases. The latest version of the ROCm package, now includes <a href="https://github.com/ROCmSoftwarePlatform/hipfort" target="_blank">hipfort</a>, a Fortran interface that exposes the HIP API through ISO C Binding. <a href="https://rocmdocs.amd.com/en/latest/Programming_Guides/Opencl-programming-guide.html" target="_blank">OpenCL</a> is framework that is available through a C runtime API and is supported by AMD, Intel, and Nvidia GPUs and x86 CPUs. The ROCm platform provides an OpenCL runtime environment necessary for building portable, parallel applications that run on a variety of platforms.</p>

<h4 is-upgraded>OpenMP 5.0</h4>

<p>For Directive-based GPU programming, ROCm includes the <a href="https://github.com/ROCm-Developer-Tools/aomp" target="_blank">AOMP compilers</a> for C/C++ and Fortran. The AOMP compilers are an extension of the LLVM-based Clang and Flang compilers that support the OpenMP 5.0 standard for multi-core CPU and GPU programming on both AMD and Nvidia GPUs.</p>

<h4 is-upgraded>GPU Accelerated Libraries</h4>

<p>In addition to the programming APIs, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html" target="_blank">ROCm includes portable accelerated HPC and Machine Learning libraries</a>, such as  <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocblas" target="_blank">rocBLAS</a>, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocfft" target="_blank">rocFFT</a>, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocthrust" target="_blank">rocThrust</a>, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#hipsparse" target="_blank">rocSparse</a>, <a href="https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#tensorflow" target="_blank">Tensorflow</a>, <a href="https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#pytorch" target="_blank">PyTorch</a>, <a href="https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#miopen" target="_blank">MIOpen</a>, and many others. All of these tools are provided under open-source licensing and made freely available to help you accelerate your time-to-science in a community driven ecosystem. These libraries are beneficial when you want to quickly and optimally leverage GPUs, without having to write GPU kernels yourself.</p>

<h2 is-upgraded><strong>What you will build</strong></h2>

<p>In this codelab, we will focus on how to accelerate an application in C with OpenMP 5.0. You are going to work through transitioning a serial CPU-only mini-application to a portable GPU accelerated application, using OpenMP provided through the AOMP compilers. </p>

<h2 is-upgraded><strong>What you will learn</strong></h2>

<ul>

<li>How to develop a GPU porting strategy using application profiles and call graphs.</li>

<li>How to manage GPU memory with HIP.</li>

<li>How to launch GPU accelerated kernels with OpenMP.</li>

<li>How to build GPU accelerated C/C++ applications for AMD and Nvidia platforms with a simple Makefile.</li>

<li>How to verify GPU memory allocation and kernel execution with the rocprof profiler.</li>

</ul>

<h2 is-upgraded><strong>What you will need</strong></h2>

<ul>

<li>A compute platform with AMD or Nvidia GPU(s)</li>

<li>CUDA Toolkit 10 or greater (Nvidia platforms only)</li>

<li>Linux operating system (e.g. Debian, Ubuntu, CentOS, or RHEL)</li>

<li>Working installation of the <a href="https://github.com/ROCm-Developer-Tools/aomp/blob/master/docs/INSTALL.md" target="_blank">AOMP compiler</a></li>

<li>Basic Command-Line Linux Experience</li>

<li>Working gcc compiler</li>

</ul>





      </google-codelab-step>

    

      <google-codelab-step label="Clone and Run the Demo Application (CPU-Only)" duration="15">

        <p>In this section, we introduce the demo application and walk through building and verifying the example. It&#39;s important to make sure that the code produces the expected result as we will be using the CPU generated model output to ensure that the solution does not change when we port to the GPU. </p>

<aside class="special"><p><strong>Tip:</strong> In practice, it&#39;s ideal to define tests for all of your routines as standalone (unit-tests) and/or in concert together (integration-tests). These tests would ideally be run regularly during development and with every commit to your code&#39;s repository.</p>

</aside>

<p>This application executes a 2-D smoothing operation on a square grid of points. The program proceeds as follows</p>

<ol type="1" start="1">

<li>Process command line arguments</li>

<li>Allocate memory for smoother class - 5x5 stencil with Gaussian weights</li>

<li>Allocate memory for function and smoothed function</li>

<li>Initialize function on CPU and report function to file</li>

<li>Call smoothing function</li>

<li>Report smoothed function to file</li>

<li>Clear memory</li>

</ol>

<h2 is-upgraded><strong>Code Structure</strong></h2>

<p>This application&#39;s src directory contains the following files</p>

<ol type="1" start="1">

<li><code>smoother.cpp</code> : Defines a simple data structure that stores the smoothing operators weights and the routines for allocating memory, deallocating memory, and executing the smoothing operation.</li>

<li><code>main.cpp</code> : Defines the main program that sets up the 2-D field to be smoothed and managed file IO.</li>

<li><code>Makefile</code> : A simple makefile is to build the application binary <code>smoother</code>.</li>

<li><code>viz.py</code> : A python script for creating plots of the smoother output</li>

</ol>

<h2 is-upgraded><strong>Install and Verify the Application</strong></h2>

<p>To get started, we want to make sure that the application builds and runs on your system using the gcc compiler. Once verified, you will modify the provided Makefile to use the AOMP compiler and verify the results.</p>

<ol type="1" start="1">

<li>Clone the repository</li>

</ol>

<pre><code>$ git clone https://github.com/os-hackathon/amd-rocm-codelabs_example-codes</code></pre>

<ol type="1" start="2">

<li>Build the smoother application. Keep in mind, the compiler is set to gcc by default in the provided makefile.</li>

</ol>

<pre><code>$ cd c++/smoother/src

$ make</code></pre>

<ol type="1" start="3">

<li>Test run the example. The application takes two arguments. The first argument is the number of grid cells, and the second argument is the number of times the smoothing operator is applied.</li>

</ol>

<p>$ ./smoother 1000 100</p>

<h2 is-upgraded><strong>Visualize the output (Optional)</strong></h2>

<p>You can visualize the output with the provided <code>viz.py</code> python script. We recommend using virtual environments to install the script&#39;s dependencies</p>

<ol type="1" start="1">

<li>Start a virtual environment</li>

</ol>

<pre><code>$ python3 -m venv env

$ source env/bin/activate</code></pre>

<ol type="1" start="2">

<li>Install the required packages</li>

</ol>

<pre><code>(env)$ pip3 install -r requirements.txt</code></pre>

<ol type="1" start="3">

<li>Execute viz.py</li>

</ol>

<pre><code>(env)$ python3 ./viz.py</code></pre>

<p>This script saves a figure to <code>function.eps</code>. This figure shows the initial 2-D function before smoothing on the top and the smoothed field on the bottom. An example of the visualized output from the <code>smoother</code> example program is shown in the image below for a grid with 100x100 cells. The initial field is shown on the top, and the smoothed field is shown on the bottom after 100 iterations. Increasing the number of iterations (the second argument) will enhance the amount of smoothing and will further blur the image.<img style="width: 624.00px" src="img/36a5cdd828d91bbd.png"></p>

<h2 is-upgraded><strong>Profile the Application</strong></h2>

<p>Before starting any GPU porting exercise, it is important to profile your application to find hotspots where your application spends most of its time. Further, it is helpful to keep track of the runtime of the routines in your application so that you can later assess whether or not the GPU porting has resulted in improved performance. Ideally, your GPU-Accelerated application should outperform CPU-Only versions of your application when fully subscribed to available CPUs on a compute node.</p>

<aside class="special"><p><strong>Tip:</strong> In practice, you will want to compare the run-time between fully-subscribed CPU-only routines and the GPU-ported routines to obtain a fair comparison between CPU-Only and GPU-Accelerated versions of your application.</p>

<p>If your application is not parallelized on the CPU, you can estimate the idealized runtime on the CPU by dividing the serial runtime by the number of cores available on your target hardware.</p>

</aside>

<p>There are a number of open-source tools available for profiling C/C++ and Fortran applications, including <a href="https://www.cs.uoregon.edu/research/tau/home.php" target="_blank">Tau</a>, <a href="https://www.vi-hps.org/projects/score-p" target="_blank">Score-P</a>, <a href="https://vampir.eu/" target="_blank">Vampir</a>, and <a href="https://www.scalasca.org/" target="_blank">Scalasca</a>. In this tutorial, we are going to generate a profile and call graph using gprof. </p>

<h3 is-upgraded><strong>Create the profile</strong></h3>

<ol type="1" start="1">

<li>Add -pg flag to the CFLAGS variable in the provided Makefile.</li>

</ol>

<pre><code>CFLAGS=-O0 -g -pg</code></pre>

<ol type="1" start="2">

<li>Remove files from your previous build.</li>

</ol>

<pre><code>$ make clean</code></pre>

<ol type="1" start="3">

<li>Make the smoother application</li>

</ol>

<pre><code>$ make</code></pre>

<ol type="1" start="4">

<li>Run the application and obtain an overall timing. When the -pg flag is passed to the gcc compiler, executions of the application will create a file called gmon.out</li>

</ol>

<pre><code>$ time ./smoother 1000 10</code></pre>

<ol type="1" start="5">

<li>Create the profile</li>

</ol>

<pre><code>$ gprof ./smoother gmon.out &gt; profile.txt</code></pre>

<aside class="special"><p><strong>Tip:</strong> If you are working with compilers other than GNU compilers, you can use <a href="https://developer.mantidproject.org/ProfilingWithValgrind.html" target="_blank">Valgrind&#39;s Callgrind</a> command line tool to create call-graphs and to conduct hotspot analysis.</p>

</aside>

<h3 is-upgraded><strong>Interpret the profile and callgraph</strong></h3>

<p><code>gprof</code> provides a flat profile and a summary of your application&#39;s call structure indicating dependencies within your source code as a call graph. A <strong><em>call tree</em></strong> depicts the relationships between routines in your source code. Combining timing information with a call graph will help you plan the order in which you port routines to the GPU.</p>

<p>The first section of the gprof output is the flat-profile. An example flat-profile for the <code>smoother</code> application is given below. The flat-profile provides a list of routines in your application, ordered by the percent time your program spends within those routines from greatest to least. Beneath the flat-profile, gprof provides documentation of each of the columns for your convenience.</p>

<pre><code>  %   cumulative   self              self     total           

 time   seconds   seconds    calls  ms/call  ms/call  name    

 95.24      1.16     1.16       10   116.19   116.19  smoothField

  2.46      1.19     0.03       10     3.00     3.00  resetF

  2.46      1.22     0.03                             main

  0.00      1.22     0.00        1     0.00     0.00  smootherFree

  0.00      1.22     0.00        1     0.00     0.00  smootherInit</code></pre>

<p>Let&#39;s now take a look at at the call tree. This call tree has five entries, one for each routine in our program. The right-most field for each entry indicates the routines that called each routine and that are called by each routine. </p>

<p>For <code>smoother</code>, the first entry shows that main calls <code>smoothField</code>, <code>resetF</code>, <code>smootherInit</code>, and <code>smootherFree</code>. Further, the called column indicates that smoothField and resetF routines are shown to be called 10 times (in this case) by main. The self and children columns indicate that main spends 0.03s executing instructions in main and 1.19s in calling other routines. Further, of those 1.19s, 1.16s are spent in <code>smoothField</code> and 0.03 are spent in <code>resetF</code>. </p>

<pre><code>index % time    self  children    called     name

                                                 &lt;spontaneous&gt;

[1]    100.0    0.03    1.19                 main [1]

                1.16    0.00      10/10          smoothField [2]

                0.03    0.00      10/10          resetF [3]

                0.00    0.00       1/1           smootherInit [5]

                0.00    0.00       1/1           smootherFree [4]

-----------------------------------------------

                1.16    0.00      10/10          main [1]

[2]     95.1    1.16    0.00      10         smoothField [2]

-----------------------------------------------

                0.03    0.00      10/10          main [1]

[3]      2.5    0.03    0.00      10         resetF [3]

-----------------------------------------------

                0.00    0.00       1/1           main [1]

[4]      0.0    0.00    0.00       1         smootherFree [4]

-----------------------------------------------

                0.00    0.00       1/1           main [1]

[5]      0.0    0.00    0.00       1         smootherInit [5]

-----------------------------------------------</code></pre>

<aside class="special"><p><strong>Tip:</strong> You can use the open-source <a href="https://github.com/jrfonseca/gprof2dot" target="_blank">gprof2dot</a> to create visualizations of gprof output to help interpret the profile and call-graph for more complex applications.</p>

</aside>

<h3 is-upgraded><strong>Next steps</strong></h3>

<p>Now that we have a profile and an understanding of the call structure of the application, we can now plan our port to GPUs. Since we will use the AOMP compiler for offloading to GPUs, we want to first modify the Makefile to use the AOMP compiler. Then, we will focus on porting the smoothField routine and the necessary data to the GPU, since smoothField takes up the majority of the run time. </p>

<p>When we port this routine, we will introduce data allocation on the GPU and data copies between CPU and GPU. This data movement may potentially increase the overall application runtime, even if the smoothField routine performs better. In this event, we will then work on minimizing data movements between CPU and GPU. </p>

<aside class="special"><p class="image-container"><img style="width: 231.82px" src="img/fb9a02e8f06a74cb.png"></p>

<p><strong>Tip:</strong> As a general strategy, it is recommended that you approach GPU porting in small incremental steps. Each step should consist of (1) profiling, (2) planning, (3) implementing planned changes &amp; verifying the application output, and (4) committing the changes to your repository.</p>

</aside>





      </google-codelab-step>

    

      <google-codelab-step label="Update the Makefile to use the AOMP compiler" duration="15">

        <p>Now that you&#39;ve verified the smoother example application installs and runs successfully using the GCC compiler, you will make the transition to using the AOMP compiler. The AOMP compiler uses a branch of clang under-the-hood that is OpenMP 5.0 compliant. Before jumping straight into GPU offloading with OpenMP, you will take an incremental step to change the compiler and verify the application can be compiled and executed with the AOMP compiler. Once this is verified, you will then start the GPU offloading process.</p>

<h2 is-upgraded><strong>Update the Makefile</strong></h2>

<aside class="special"><p><strong>Tip:</strong> When transitioning to OpenMP 5.0 with AOMP, your application&#39;s make system will need to be updated to determine where AOMP is installed. Additionally, it is helpful to determine the target architecture you will be compiling for. <a href="https://github.com/ROCm-Developer-Tools/aomp/blob/master/examples/openmp/vmulsum/Makefile" target="_blank">The AOMP Github Repository provides a number of good Makefile examples to help you get started</a>.</p>

</aside>

<ol type="1" start="1">

<li>Starting from the <code>smoother</code> makefile (<code>smoother/src/Makefile</code>), insert the following block of code at the top of the makefile to help detect the location of AOMP. This block of code is borrowed from <a href="https://github.com/ROCm-Developer-Tools/aomp/blob/master/examples/openmp/vmulsum/Makefile" target="_blank">an AOMP example Makefile</a>. Notice that this block set the C compiler to clang provided by AOMP.</li>

</ol>

<pre><code>CFLAGS=-O0 -g

LFLAGS=-lm



ifeq (&#34;$(wildcard $(AOMP))&#34;,&#34;&#34;)

  ifneq ($(AOMP),)

    $(warning AOMP not found at $(AOMP))

  endif

  AOMP = $(HOME)/rocm/aomp

  ifeq (&#34;$(wildcard $(AOMP))&#34;,&#34;&#34;)

    $(warning AOMP not found at $(AOMP))

    AOMP = /usr/lib/aomp

    ifeq (&#34;$(wildcard $(AOMP))&#34;,&#34;&#34;)

      $(warning AOMP not found at $(AOMP))

      $(error Please install AOMP or correctly set env-var AOMP)

    endif

  endif

Endif



CC = $(AOMP)/bin/clang</code></pre>

<ol type="1" start="2">

<li>The AOMP installation provides a bash utility called <code>mygpu</code> that can help detect the GPU on your system. Once the AOMP path is known, you can use the <code>mygpu</code> script to detect the GPU target to offload to.</li>

</ol>

<pre><code>INSTALLED_GPU  = $(shell $(AOMP)/bin/mygpu -d gfx900)

AOMP_GPU       ?= $(INSTALLED_GPU)



ifeq (sm_,$(findstring sm_,$(AOMP_GPU)))

  AOMP_GPUTARGET = nvptx64-nvidia-cuda

else

  AOMP_GPUTARGET = amdgcn-amd-amdhsa

endif</code></pre>

<ol type="1" start="3">

<li>For offloading sections of code to the CPU, you may also want to add a section to the Makefile that detects the CPU platform you are working on.</li>

</ol>

<pre><code>UNAMEP = $(shell uname -p)

AOMP_CPUTARGET = $(UNAMEP)-pc-linux-gnu

ifeq ($(UNAMEP),ppc64le)

  AOMP_CPUTARGET = ppc64le-linux-gnu

endif</code></pre>

<ol type="1" start="4">

<li>Next, you can append the OpenMP offload flags to the CFLAGS environment variable.</li>

</ol>

<pre><code>CFLAGS += -target $(AOMP_CPUTARGET) -fopenmp -fopenmp-targets=$(AOMP_GPUTARGET) -Xopenmp-target=$(AOMP_GPUTARGET) -march=$(AOMP_GPU)</code></pre>

<ol type="1" start="5">

<li>If you are working on an Nvidia platform, you will need to add the CUDA runtime library (<code>-lcudart</code>) to the <code>LFLAGS</code> variable.</li>

</ol>

<pre><code>ifeq (sm_,$(findstring sm_,$(AOMP_GPU)))

  CUDA   ?= /usr/local/cuda

  LFLAGS += -L$(CUDA)/targets/$(UNAMEP)-linux/lib -lcudart

endif</code></pre>

<aside class="warning"><p><strong>Caution: </strong>Since this makefile is using commands to detect the CPU and GPU platforms, you will need to compile the application on the platform you plan to run the application. Optionally, you can substitute the code blocks in Step 2 and 3 with code that could allow you to set the <code>INSTALLED_GPU</code> and <code>AOMP_CPUTARGET</code> through environment variables.</p>

</aside>

<h2 is-upgraded><strong>Verify the application compiles and runs</strong></h2>

<p>Now that you have made the necessary modifications to the Makefile, it is time to re-compile and test the application. You also want to make sure that the application output is unchanged.</p>

<ol type="1" start="1">

<li>Copy the existing output from your previous run to a reference directory.</li>

</ol>

<pre><code>$ mkdir reference

$ cp function.txt smooth-function.txt reference/</code></pre>

<ol type="1" start="2">

<li>Re-compile the <code>smoother</code> application.</li>

</ol>

<pre><code>$ make clean

$ make</code></pre>

<ol type="1" start="3">

<li>Run the <code>smoother</code> application with the same input parameters as before and compare the output with the reference output. You can use the diff command line tool to compare the new output with the reference output. If the files are identical, no output will be printed to screen.</li>

</ol>

<pre><code>$ ./smoother 1000 10

$ diff function.txt reference/function.txt

$ diff smooth-function.txt reference/function.txt</code></pre>

<h2 is-upgraded><strong>Next Steps</strong></h2>

<p>Now that you&#39;ve switched to using the AOMP compiler and have verified the application successfully compiles and runs and produces the correct output, you are ready to begin offloading to GPUs with OpenMP. In the next step, you will offload the <code>smoothField</code> and <code>resetF</code> routines using OpenMP directives.</p>





      </google-codelab-step>

    

      <google-codelab-step label="Offload Routines to the GPU with OpenMP" duration="20">

        <p>In the <code>smoother</code> application, we have seen that the <code>smoothField</code> routine, called by <code>main</code>, takes up the most time. Within the main iteration loop in <code>main.cpp</code>, the <code>resetF</code> function is called to update the input for <code>smoother</code> for the next iteration.</p>

<p>You will start by offloading both the <code>smoothField</code> and <code>resetF</code> routines to the GPU using OpenMP directives (also called &#34;pragmas&#34;). In this section you will learn how to offload sections of code to the GPU and how to manage GPU data using OpenMP pragmas. </p>

<h2 is-upgraded><strong>OMP Target Basics</strong></h2>

<p>With OpenMP, you can use the <code>omp target</code> directive to mark regions of code that you want the compiler to offload to the GPU. When you open an OpenMP target region, you can use the <strong><code>map</code></strong> directive to indicate variables that you want to copy <strong><code>to</code></strong> the GPU and <strong><code>from</code></strong> the GPU. </p>

<p>The example below shows how to open and close a target region that will be offloaded to the GPU. Pointers are created for variables <code>arrayIn</code> and <code>arrayOut</code> on the host. The map directives indicates that <code>arrayIn</code> will be copied to the GPU and <code>arrayOut</code> will be copied from the GPU to the CPU at the end of the target region. Note that, since <code>arrayIn</code> and <code>arrayOut</code> are pointers, we must use array section notation to properly map the arrays.</p>

<pre><code>int N = 1000;

float *arrayIn;

float *arrayOut;

arrayIn = (float*)malloc( N*sizeof(float) );

arrayOut = (float*)malloc( N*sizeof(float) );



// Initialization routines ... //

.

.

// End Initialization routines

#omp pragma target map (to: arrayIn[0:N]) map (from: arrayOut[0:N])

{

.

.

}</code></pre>

<h2 is-upgraded><strong>OMP Teams &amp; Parallel</strong></h2>

<p>Within a target region, with no other specifications, a single thread of execution is launched on the GPU. However, GPUs are capable of running thousands of threads simultaneously. Threads on GPUs are scheduled to run on multiple Compute Units (AMD) or Streaming Multiprocessors (Nvidia) in groups of 64 (AMD) or 32 (Nvidia) called Wavefronts (AMD) or Warps (Nvidia). Modern GPUs are capable of executing many Wavefronts/Warps at any given time. </p>

<p>Before the teams directive was introduced in OpenMP 4.0, parallelization was limited to parallelizing with a single group of threads. Since OpenMP 4.0, the teams directive can be used to express another dimension of parallelism that is appropriate for GPUs.</p>

<p>The teams directive creates a &#34;league&#34; of teams that have, by default, a single thread. Each team executes instructions concurrently. The parallel directive creates multiple threads within each team. The number of threads can be set with the optional <code>num_threads</code> clause after the parallel directive. On a GPU, the number of threads per team is ideally a multiple of the Wavefront/Warp size (64 or 32 threads).</p>

<p>This example shows how to parallelize a for-loop with parallel teams, where each team has 256 threads.</p>

<pre><code>int N = 1000;

float *arrayIn;

float *arrayOut;

arrayIn = (float*)malloc( N*sizeof(float) );

arrayOut = (float*)malloc( N*sizeof(float) );





#omp pragma target map (to: arrayIn[0:N]) map (from: arrayOut[0:N])

{

  #pragma omp teams parallel for num_threads(256)

  for( int i = 0; i&lt;N; i++){

    arrayOut[i] = 2.0*arrayIn[i];

  }

}</code></pre>

<h2 is-upgraded>Offload smoothField</h2>

<ol type="1" start="1">

<li>Open <code>smoother.c</code> and navigate to the <code>smoothField</code> routine. Open an OpenMP target region before the start of the first loop in <code>smoothField</code> and map the necessary map directives to copy <code>smoother-&gt;weights</code> and <code>f</code> to the GPU and <code>smoothF</code> to and from the GPU.</li>

</ol>

<pre><code>  #pragma omp target  map(to:smoothOperator-&gt;weights[0:N*N], f[0:nX*nY]) map(smoothF[0:nX*nY])</code></pre>

<ol type="1" start="2">

<li>Use a <code>teams parallel for</code> directive with a <code>collapse(2)</code> clause to parallelize the outer two loops. </li>

</ol>

<pre><code>  #pragma omp target  map(to:smoothOperator-&gt;weights[0:N*N], f[0:nX*nY]) map(smoothF[0:nX*nY])

  {

    #pragma omp teams distribute parallel for collapse(2) num_threads(256)

    for( int j=buf; j &lt; nY-buf; j++ ){

      for( int i=buf; i &lt; nX-buf; i++ ){

        smLocal = 0.0;

        for( int jj=-buf; jj &lt;= buf; jj++ ){

          for( int ii=-buf; ii &lt;= buf; ii++ ){

            iloc = (i+ii)+(j+jj)*nX;

            ism = (ii+buf) + (jj+buf)*N;

            smLocal += f[iloc]*smoothOperator-&gt;weights[ism];

          }

        }

        iel = i+j*nX;

        smoothF[iel] = smLocal;

      }

    }

  }</code></pre>

<ol type="1" start="3">

<li>Re-compile the <code>smoother</code> application.</li>

</ol>

<pre><code>$ make</code></pre>

<ol type="1" start="4">

<li>Run the <code>smoother</code> application with the same input parameters as before and compare the output with the reference output. You can use the diff command line tool to compare the new output with the reference output. If the files are identical, no output will be printed to screen.</li>

</ol>

<pre><code>$ time ./smoother 1000 10

real        0m2.767s

user        0m1.568s

sys        0m1.057s

$ diff function.txt reference/function.txt

$ diff smooth-function.txt reference/function.txt</code></pre>

<h2 is-upgraded>Offload resetF</h2>

<ol type="1" start="1">

<li>Open <code>smoother.cpp</code> and navigate to the <code>resetF</code> routine. Open an OpenMP <code>target</code> region before the start of the first loop in <code>resetF</code> and map the necessary map directives to copy <code>smoothF</code> to the GPU and <code>f</code> to and from the GPU.</li>

</ol>

<pre><code>  #pragma omp target map(to: smoothF[0:nx*ny]) map(f[0:nx*ny])</code></pre>

<ol type="1" start="2">

<li>Use a <code>teams parallel for</code> directive with a <code>collapse(2)</code> clause to parallelize the outer two loops. </li>

</ol>

<pre><code>  #pragma omp target map(to: smoothF[0:nx*ny]) map(f[0:nx*ny])

  {

    #pragma omp teams distribute parallel for collapse(2) num_threads(256)

    for( int iy=buf; iy&lt;ny-buf; iy++ ){

      for( int ix=buf; ix&lt;nx-buf; ix++ ){

        iel = ix + nx*iy;

        f[iel] = smoothF[iel];

      }

    }

  }</code></pre>

<ol type="1" start="3">

<li>Re-compile the <code>smoother</code> application.</li>

</ol>

<pre><code>$ make</code></pre>

<ol type="1" start="4">

<li>Run the <code>smoother</code> application with the same input parameters as before and compare the output with the reference output. You can use the diff command line tool to compare the new output with the reference output. If the files are identical, no output will be printed to screen.</li>

</ol>

<pre><code>$ time ./smoother 1000 10

real        0m2.970s

user        0m1.528s

sys        0m1.228s

$ diff function.txt reference/function.txt

$ diff smooth-function.txt reference/function.txt</code></pre>

<ol type="1" start="5">

<li>You can profile the application using the rocprof profiler. To profile OpenMP accelerated applications, you will need to set the flags <code>--hsa-trace --obj-tracking on</code>. It is also helpful to enable the summary statistics using the <code>--stats</code> flag.</li>

</ol>

<pre><code>$ rocprof --hsa-trace --obj-tracking on --stats ./smoother 1000 10

$ cat results.stat.csv

&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;

&#34;__omp_offloading_801_440b81_smoothField_l67.kd&#34;,10,30603997,3060399,78.4420113965

&#34;__omp_offloading_801_440b81_resetF_l48.kd&#34;,10,8410807,841080,21.5579886035</code></pre>

<aside class="special"><p><strong>Tip:</strong> You can use <a href="https://www.chromium.org/developers/how-tos/trace-event-profiling-tool" target="_blank">Google Chrome Tracing</a> to visualize the results.json trace profile. Simply open the Google Chrome web browser and navigate to chrome://tracing and upload results.json. </p>

<p class="image-container"><img style="width: 610.00px" src="img/2a8256846130c942.png"></p>

</aside>

<h2 is-upgraded><strong>Next steps</strong></h2>

<p>You&#39;ve successfully offloaded two routines to the GPU. However, you may have noticed that the runtime did not improve much, and may have even gotten worse, after you offloaded the second routine (<code>resetF</code>). At the start and end of each target region, the application is copying data between the CPU and GPU. You can see this behavior in the trace profile shown above. Ideally, you want to minimize data movement between the host and device for optimal performance.</p>

<p>In the next section, you will learn how to control when data is allocated and moved to and from the GPU. This will help you minimize data copies between the host and device that often become bottlenecks for GPU accelerated applications.</p>





      </google-codelab-step>

    

      <google-codelab-step label="Using Unstructured Data Directives" duration="15">

        <p>In this section you will learn how to use unstructured data directives with OpenMP to control when data is copied to and from the GPU. </p>

<p>In the <code>smoother</code> application, there are two routines within a main iteration loop, <code>smoothField</code> and <code>resetF</code>. Both routines operate on data stored in two arrays, <code>f</code> and <code>smoothF</code>. </p>

<pre><code>  for( int iter=0; iter&lt;nIter; iter++){

    // Run the smoother

    smoothField( &amp;smoothOperator, f, smoothF, nx, ny );

    // Reassign smoothF to f

    resetF( f, smoothF, nx, ny, buf );

  }</code></pre>

<p>Additionally, the <code>smoothField</code> routine requires the <code>smoothOperator-&gt;weights</code> array in order to calculate <code>smoothF</code> from <code>f</code>. Currently, target regions within smoothField and resetF copy these arrays to and from the GPU, before and after executing the routine instructions in parallel on the GPU; this is also done every iteration. </p>

<h2 is-upgraded><strong>OMP Enter/Exit Data</strong></h2>

<p>Ideally, we want to have all of the necessary data copied to the GPU before the iteration loop and have <code>smoothF</code> copied from the GPU after the iteration loop. This can be achieved using the <code>target enter data</code> and <code>target exit data</code> directives.</p>

<p>Each directive is a standalone directive that can be used to allocate or deallocate memory on the GPU and copy data to or from the GPU. A typical usage is to use the target enter data directive to allocate device memory after allocation on the host and to use the target exit data directive to free device memory before freeing memory on the host. Then, you can use the target update directive to manage updating host and device data when needed.</p>

<p>In this example below, the <code>enter data directive</code> is used to allocate device memory for <code>arrayIn</code> and <code>arrayOut</code>. Before reaching the main block of code, the <code>target update directive</code> is used to update <code>arrayIn</code> on the device. At the end of this region of code, the <code>target update directive</code> is used to update <code>arrayOut</code> on the host. At the end of the example code, the <code>exit data directive</code> is used to free device memory before freeing the associate host pointer.</p>

<pre><code>int N = 1000;

float *arrayIn;

float *arrayOut;

arrayIn = (float*)malloc( N*sizeof(float) );

arrayOut = (float*)malloc( N*sizeof(float) );

# omp pragma target enter data map(alloc:arrayIn[0:N], arrayOut[0:N])



// Initialization routines ... //

.

.

// End Initialization routines

#omp pragma target update to(arrayIn[0:N]) 

{

// Execution block

.

.

}

#omp pragma target update from(arrayOut[0:N]) 

.

.

# omp pragma target exit data map(delete:arrayIn[0:N], arrayOut[0:N])

free(arrayIn);

free(arrayOut);</code></pre>

<h2 is-upgraded><strong>Transition to enter/exit data directives</strong></h2>

<p>In the smoother application, we want to explicitly control data movement for <code>f</code>, <code>smoothF</code>, and <code>smoothOperator-&gt;weights</code>. You will work in <code>main.cpp</code> to insert calls to allocate, update, and deallocate device memory for <code>f</code> and <code>smoothF</code>. To handle <code>smoothOperator-&gt;weights</code>, you will work in <code>smoother.cpp</code> to allocate, update, and deallocate device memory.</p>

<ol type="1" start="1">

<li>Open main.cppand find where <code>f</code> and <code>smoothF</code> are allocated memory. Just after the <code>malloc</code> calls, add a <code>target enter data</code> directive to allocate device memory for <code>f</code> and smoothF.</li>

</ol>

<pre><code>  // Allocate space for the function we want to smooth

  f  = (real*)malloc( nElements*sizeof(real) );

  smoothF = (real*)malloc( nElements*sizeof(real) );

  #pragma omp target enter data map(alloc: f[0:nElements], smoothF[0:nElements])</code></pre>

<ol type="1" start="2">

<li>Add a <code>target update to</code> directive to copy <code>f</code> and <code>smoothF</code> data to the GPU just before the main iteration loop and add a <code>target update from</code> directive to copy <code>smoothF</code> from the GPU just after the main iteration loop.</li>

</ol>

<pre><code>  #pragma omp target update to(f[0:nElements], smoothF[0:nElements])

  for( int iter=0; iter&lt;nIter; iter++){



    // Run the smoother

    smoothField( &amp;smoothOperator, f, smoothF, nx, ny );



    // Reassign smoothF to f

    resetF( f, smoothF, nx, ny, buf );

  }

  #pragma omp target update from(smoothF[0:nElements])</code></pre>

<ol type="1" start="3">

<li>Add a <code>target exit data</code> directive to deallocate device memory held by <code>f</code> and smoothF before calling <code>free</code> at the end of <code>main.cpp</code>.</li>

</ol>

<pre><code>  // Free space

  #pragma omp target exit data map(delete:f[0:nElements],smoothF[0:nElements])

  free(f);

  free(smoothF);</code></pre>

<ol type="1" start="4">

<li>Save <code>main.cpp</code> and open <code>smoother.cpp</code>. In the <code>smootherInit</code> routine, add a <code>target enter data</code> directive to allocate device memory for <code>smoothOperator-&gt;weights</code>.</li>

</ol>

<pre><code>  smoothOperator-&gt;dim = N;

  smoothOperator-&gt;weights = (real*)malloc( N*N*sizeof(real) );

  #pragma omp target enter data map(alloc: smoothOperator-&gt;weights[0:N*N])</code></pre>

<ol type="1" start="5">

<li>Add a target update directive for <code>smoothOperator-&gt;weights</code> at the end of the <code>smootherInit</code> routine, after the weight values have been assigned.</li>

</ol>

<pre><code>  for( int j=0; j &lt; N; j++ ){

    for( int i=0; i &lt; N; i++ ){

      smoothOperator-&gt;weights[i+j*N] = smoothOperator-&gt;weights[i+j*N]/wsum;

    }

  }

  #pragma omp target update to(smoothOperator-&gt;weights[0:N*N])</code></pre>

<ol type="1" start="6">

<li>Add a <code>target exit data directive</code> to deallocate device memory held by <code>smoothOperator-&gt;weights</code> in the smootherFree routine.</li>

</ol>

<pre><code>void smootherFree( struct smoother *smoothOperator )

{

  #pragma omp target exit data map(delete: smoothOperator-&gt;weights[0:smoothOperator-&gt;dim*smoothOperator-&gt;dim])

  free( smoothOperator-&gt;weights );

}</code></pre>

<ol type="1" start="7">

<li>Re-compile the <code>smoother</code> application.</li>

</ol>

<pre><code>$ make</code></pre>

<ol type="1" start="8">

<li>Run the <code>smoother</code> application with the same input parameters as before and compare the output with the reference output. You can use the diff command line tool to compare the new output with the reference output. If the files are identical, no output will be printed to screen.</li>

</ol>

<pre><code>$ time ./smoother 1000 10

real        0m2.689s

user        0m1.496s

sys        0m1.052s

$ diff function.txt reference/function.txt

$ diff smooth-function.txt reference/function.txt</code></pre>





      </google-codelab-step>

    

      <google-codelab-step label="Congratulations" duration="0">

        <p>In this codelab, you learned how to port serial CPU-only routines in C to GPUs using OpenMP. To do this, you used target directives to offload regions of code to the GPU. You used <code>teams parallel for</code> directives to parallelize nested loops across teams of SIMD threads. </p>

<p>To reduce data copies between host and device, you applied unstructured OpenMP data directives to explicitly manage when memory is allocated/deallocated on the GPU and when data is copied between to and from the GPU.</p>

<p>In the process of doing this, you practiced a strategy for porting to GPUs that included the following steps to make incremental changes to your own source code :</p>

<ol type="1" start="1">

<li>Profile - Find out the hotspots in your code and understand the dependencies with other routines</li>

<li>Plan - Determine what routine you want to port and what data needs to be copied to and from the GPU.</li>

<li>Implement &amp; Verify - Insert the necessary OpenMP directives, compile the application, and verify the results.</li>

<li>Commit - Once you have verified correctness and the expected behavior, commit your changes and start the process over again.</li>

</ol>

<h2 is-upgraded><strong>Provide Feedback</strong></h2>

<p>If you have any questions, comments, or feedback that can help improve this codelab, you can <a href="https://github.com/os-hackathon/amd-rocm-codelabs/issues/new" target="_blank"><strong>open an issue on the os-hackathon/amd-rocm-codelabs Github repository</strong></a>.</p>

<h2 is-upgraded><strong>Further reading</strong></h2>

<p><a href="https://www.openmp.org/wp-content/uploads/openmp-examples-5.0.0.pdf" target="_blank">OpenMP 5.0 Examples Documentation</a></p>





      </google-codelab-step>

    

  </google-codelab>



  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>

  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>

  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>

  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>



</body>

</html>

